{"ast":null,"code":"// Note: Web-based speech recognition is limited compared to server-side solutions\n\nexport const transcribeAudio = async audioFile => {\n  // For audio file transcription, we'd typically need to send the file to a backend service\n  // since browser APIs are limited for handling audio files\n\n  // As a fallback for demonstration, we'll check if we can use the browser's SpeechRecognition API\n  if (audioFile instanceof File) {\n    // Check if the Web Speech API is available for audio processing\n    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (!SpeechRecognition) {\n      return \"Your browser doesn't support speech recognition. To use actual transcription, you would need to implement a server-side solution with a service like Google Speech-to-Text.\";\n    }\n\n    // In a real implementation, we would send this file to a backend API\n    return \"This is a demonstration. In a production environment, the audio would be sent to a backend server that uses a professional transcription API. Sample transcript: 'Today we discussed the implementation of speech recognition in web applications and its limitations in browser environments.'\";\n  }\n\n  // For live transcription (not from a file)\n  return new Promise((resolve, reject) => {\n    // Check if browser supports speech recognition\n    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (!SpeechRecognition) {\n      reject(new Error(\"Your browser doesn't support speech recognition. Try a different browser or upload an audio file.\"));\n      return;\n    }\n    const recognition = new SpeechRecognition();\n    recognition.lang = \"en-US\";\n    recognition.continuous = true;\n    recognition.interimResults = false;\n    let finalTranscript = \"\";\n    recognition.onresult = event => {\n      for (let i = event.resultIndex; i < event.results.length; i++) {\n        if (event.results[i].isFinal) {\n          finalTranscript += event.results[i][0].transcript + \" \";\n        }\n      }\n    };\n    recognition.onerror = event => {\n      recognition.stop();\n      reject(new Error(`Speech recognition error: ${event.error}`));\n    };\n    recognition.onend = () => {\n      if (finalTranscript.trim()) {\n        resolve(finalTranscript.trim());\n      } else {\n        // If no real transcript was captured, provide a more useful demo transcript\n        resolve(\"This is a demonstration transcript. In a real application with proper permissions and setup, this would contain your actual spoken words.\");\n      }\n    };\n    recognition.start();\n\n    // Automatically stop after 10 seconds (for short recordings)\n    setTimeout(() => {\n      recognition.stop();\n    }, 10000);\n  });\n};","map":{"version":3,"names":["transcribeAudio","audioFile","File","SpeechRecognition","window","webkitSpeechRecognition","Promise","resolve","reject","Error","recognition","lang","continuous","interimResults","finalTranscript","onresult","event","i","resultIndex","results","length","isFinal","transcript","onerror","stop","error","onend","trim","start","setTimeout"],"sources":["E:/LETU/Transcript-Notes/Transcript-Notes/react-transcript-notes/src/services/transcriptionService.js"],"sourcesContent":["// Note: Web-based speech recognition is limited compared to server-side solutions\r\n\r\nexport const transcribeAudio = async (audioFile) => {\r\n  // For audio file transcription, we'd typically need to send the file to a backend service\r\n  // since browser APIs are limited for handling audio files\r\n\r\n  // As a fallback for demonstration, we'll check if we can use the browser's SpeechRecognition API\r\n  if (audioFile instanceof File) {\r\n    // Check if the Web Speech API is available for audio processing\r\n    const SpeechRecognition =\r\n      window.SpeechRecognition || window.webkitSpeechRecognition;\r\n\r\n    if (!SpeechRecognition) {\r\n      return \"Your browser doesn't support speech recognition. To use actual transcription, you would need to implement a server-side solution with a service like Google Speech-to-Text.\";\r\n    }\r\n\r\n    // In a real implementation, we would send this file to a backend API\r\n    return \"This is a demonstration. In a production environment, the audio would be sent to a backend server that uses a professional transcription API. Sample transcript: 'Today we discussed the implementation of speech recognition in web applications and its limitations in browser environments.'\";\r\n  }\r\n\r\n  // For live transcription (not from a file)\r\n  return new Promise((resolve, reject) => {\r\n    // Check if browser supports speech recognition\r\n    const SpeechRecognition =\r\n      window.SpeechRecognition || window.webkitSpeechRecognition;\r\n\r\n    if (!SpeechRecognition) {\r\n      reject(\r\n        new Error(\r\n          \"Your browser doesn't support speech recognition. Try a different browser or upload an audio file.\"\r\n        )\r\n      );\r\n      return;\r\n    }\r\n\r\n    const recognition = new SpeechRecognition();\r\n    recognition.lang = \"en-US\";\r\n    recognition.continuous = true;\r\n    recognition.interimResults = false;\r\n\r\n    let finalTranscript = \"\";\r\n\r\n    recognition.onresult = (event) => {\r\n      for (let i = event.resultIndex; i < event.results.length; i++) {\r\n        if (event.results[i].isFinal) {\r\n          finalTranscript += event.results[i][0].transcript + \" \";\r\n        }\r\n      }\r\n    };\r\n\r\n    recognition.onerror = (event) => {\r\n      recognition.stop();\r\n      reject(new Error(`Speech recognition error: ${event.error}`));\r\n    };\r\n\r\n    recognition.onend = () => {\r\n      if (finalTranscript.trim()) {\r\n        resolve(finalTranscript.trim());\r\n      } else {\r\n        // If no real transcript was captured, provide a more useful demo transcript\r\n        resolve(\r\n          \"This is a demonstration transcript. In a real application with proper permissions and setup, this would contain your actual spoken words.\"\r\n        );\r\n      }\r\n    };\r\n\r\n    recognition.start();\r\n\r\n    // Automatically stop after 10 seconds (for short recordings)\r\n    setTimeout(() => {\r\n      recognition.stop();\r\n    }, 10000);\r\n  });\r\n};\r\n"],"mappings":"AAAA;;AAEA,OAAO,MAAMA,eAAe,GAAG,MAAOC,SAAS,IAAK;EAClD;EACA;;EAEA;EACA,IAAIA,SAAS,YAAYC,IAAI,EAAE;IAC7B;IACA,MAAMC,iBAAiB,GACrBC,MAAM,CAACD,iBAAiB,IAAIC,MAAM,CAACC,uBAAuB;IAE5D,IAAI,CAACF,iBAAiB,EAAE;MACtB,OAAO,6KAA6K;IACtL;;IAEA;IACA,OAAO,iSAAiS;EAC1S;;EAEA;EACA,OAAO,IAAIG,OAAO,CAAC,CAACC,OAAO,EAAEC,MAAM,KAAK;IACtC;IACA,MAAML,iBAAiB,GACrBC,MAAM,CAACD,iBAAiB,IAAIC,MAAM,CAACC,uBAAuB;IAE5D,IAAI,CAACF,iBAAiB,EAAE;MACtBK,MAAM,CACJ,IAAIC,KAAK,CACP,mGACF,CACF,CAAC;MACD;IACF;IAEA,MAAMC,WAAW,GAAG,IAAIP,iBAAiB,CAAC,CAAC;IAC3CO,WAAW,CAACC,IAAI,GAAG,OAAO;IAC1BD,WAAW,CAACE,UAAU,GAAG,IAAI;IAC7BF,WAAW,CAACG,cAAc,GAAG,KAAK;IAElC,IAAIC,eAAe,GAAG,EAAE;IAExBJ,WAAW,CAACK,QAAQ,GAAIC,KAAK,IAAK;MAChC,KAAK,IAAIC,CAAC,GAAGD,KAAK,CAACE,WAAW,EAAED,CAAC,GAAGD,KAAK,CAACG,OAAO,CAACC,MAAM,EAAEH,CAAC,EAAE,EAAE;QAC7D,IAAID,KAAK,CAACG,OAAO,CAACF,CAAC,CAAC,CAACI,OAAO,EAAE;UAC5BP,eAAe,IAAIE,KAAK,CAACG,OAAO,CAACF,CAAC,CAAC,CAAC,CAAC,CAAC,CAACK,UAAU,GAAG,GAAG;QACzD;MACF;IACF,CAAC;IAEDZ,WAAW,CAACa,OAAO,GAAIP,KAAK,IAAK;MAC/BN,WAAW,CAACc,IAAI,CAAC,CAAC;MAClBhB,MAAM,CAAC,IAAIC,KAAK,CAAC,6BAA6BO,KAAK,CAACS,KAAK,EAAE,CAAC,CAAC;IAC/D,CAAC;IAEDf,WAAW,CAACgB,KAAK,GAAG,MAAM;MACxB,IAAIZ,eAAe,CAACa,IAAI,CAAC,CAAC,EAAE;QAC1BpB,OAAO,CAACO,eAAe,CAACa,IAAI,CAAC,CAAC,CAAC;MACjC,CAAC,MAAM;QACL;QACApB,OAAO,CACL,2IACF,CAAC;MACH;IACF,CAAC;IAEDG,WAAW,CAACkB,KAAK,CAAC,CAAC;;IAEnB;IACAC,UAAU,CAAC,MAAM;MACfnB,WAAW,CAACc,IAAI,CAAC,CAAC;IACpB,CAAC,EAAE,KAAK,CAAC;EACX,CAAC,CAAC;AACJ,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}